{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# torch package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# dataset packagea\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# metrcis\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ETC\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        [(H_in + 2P - K)/S] + 1\n",
    "        '''\n",
    "\n",
    "        # (3, 224, 224) -> (64, 112, 112)\n",
    "        self.layer1 = nn.Sequential(\\\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        # (64, 112, 112) -> (128, 56, 56)\n",
    "        self.layer2 = nn.Sequential(\\\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        # (128, 56, 56) -> (256, 28, 28)\n",
    "        self.layer3 = nn.Sequential(\\\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "\n",
    "        # (256, 28, 28) -> (512, 14, 14)\n",
    "        self.layer4 = nn.Sequential(\\\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        \n",
    "        # (512, 14, 14) -> (512, 7, 7)\n",
    "        self.layer5 = nn.Sequential(\\\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Sequential(\\\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\\\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.fc3 = nn.Linear(4096, 10) # 원래는 1000 이지만 데이터셋을 CIFAR10 으로 해서 마지막 출력 노드가 10으로 설정\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        vgg16_output = self.fc3(out)\n",
    "\n",
    "        return vgg16_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, config, train, valid):\n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    for i in range(config.epochs):\n",
    "\n",
    "        train_loss_ = 0\n",
    "        model.train()\n",
    "        with torch.enable_grad():\n",
    "            for feature, label in train:\n",
    "                feature, label = feature.to(config.device), label.to(config.device)\n",
    "                optimizer.zero_grad()               \n",
    "                outputs = model(feature)\n",
    "                loss = loss_function(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss_ += loss.item()\n",
    "\n",
    "        train_loss_ /= config.batch_size\n",
    "        train_loss_history.append(train_loss_)\n",
    "\n",
    "\n",
    "        valid_loss_ = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for feature, label in valid:\n",
    "                feature, label = feature.to(config.device), label.to(config.device)\n",
    "                outputs = model(feature)\n",
    "                loss = loss_function(outputs, label)\n",
    "                \n",
    "                valid_loss_ += loss.item()\n",
    "\n",
    "        valid_loss_ /= config.batch_size\n",
    "        valid_loss_history.append(valid_loss_)\n",
    "\n",
    "        clear_output(wait=True) # 주피터 셀 초기화\n",
    "        \n",
    "        if (i+1)%int((config.epochs)*0.1)==0:\n",
    "            print(f\"epoch : {i+1} Loss(train) : {train_loss_history[-1]:.3f}  Loss(valid) : {valid_loss_history[-1]:.3f}\")\n",
    "            plt.plot(train_loss_history, label='Training loss')\n",
    "            plt.plot(valid_loss_history, label='Validation loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"=\"*40)\n",
    "    print(\"Training loss: \", train_loss_history[-1])\n",
    "    print(\"Validation loss: \", valid_loss_history[-1])\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_evaluate(model, config, test):\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    acc = []\n",
    "\n",
    "    y_pred = torch.Tensor()\n",
    "    y_test = torch.Tensor()\n",
    "    with torch.no_grad():\n",
    "        for feature, label in test:\n",
    "            feature, label = feature.to(config.device), label.to(config.device)\n",
    "            outputs = model(feature)\n",
    "            output_list.append(outputs)\n",
    "            outputs = outputs.cpu()\n",
    "            _, outputs = torch.max(outputs, 1)\n",
    "            \n",
    "            label = label.cpu()\n",
    "\n",
    "            acc.append(accuracy_score(label, outputs))\n",
    "            y_pred = torch.cat((y_pred, outputs), dim=0)\n",
    "            y_test = torch.cat((y_test, label), dim=0)\n",
    "\n",
    "    \n",
    "    print(f\"Accuracy: {(sum(acc)/len(acc)):.3f}\")\n",
    "\n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self, device, learning_rate=0.001, epochs=100, batch_size = 128):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # 48000\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True) # 12000 \n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config(device = device,\n",
    "                    learning_rate = 0.05,\n",
    "                    epochs = 100,\n",
    "                    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = SimpleVGG16().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(vgg16, train_config, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.100\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = predict_evaluate(vgg16, train_config, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.10      1.00      0.18      1000\n",
      "         1.0       0.00      0.00      0.00      1000\n",
      "         2.0       0.00      0.00      0.00      1000\n",
      "         3.0       0.00      0.00      0.00      1000\n",
      "         4.0       0.00      0.00      0.00      1000\n",
      "         5.0       0.00      0.00      0.00      1000\n",
      "         6.0       0.00      0.00      0.00      1000\n",
      "         7.0       0.00      0.00      0.00      1000\n",
      "         8.0       0.00      0.00      0.00      1000\n",
      "         9.0       0.00      0.00      0.00      1000\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.01      0.10      0.02     10000\n",
      "weighted avg       0.01      0.10      0.02     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Xenrose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Xenrose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Xenrose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
